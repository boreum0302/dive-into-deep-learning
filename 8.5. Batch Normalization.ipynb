{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53c8d82-dd3d-40ba-8aa7-a5c2a78ce84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753ca19-e29d-4f79-a0e9-a22ca1a0f49d",
   "metadata": {},
   "source": [
    "As it turns out, quite serendipitously, batch normalization conveys all three benefits: preprocessing, numerical stability, and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d820a-5447-4ff7-8906-cf9cf202e377",
   "metadata": {},
   "source": [
    "Fixing a trained model, you might think that we would prefer using the entire dataset to estimate the mean and variance. Once training is complete, why would we want the same image to be classified differently, depending on the batch in which it happens to reside? During training, such exact calculation is infeasible because the intermediate variables for all data examples change every time we update our model. However, once the model is trained, we can calculate the means and variances of each layerâ€™s variables based on the entire dataset. Indeed this is standard practice for models employing batch normalization; thus batch normalization layers function differently in training mode (normalizing by minibatch statistics) than in prediction mode (normalizing by dataset statistics). In this form they closely resemble the behavior of dropout regularization of Section 5.6, where noise is only injected during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473db18-c77c-41da-920f-fbf954e98ee0",
   "metadata": {},
   "source": [
    "The key difference from batch normalization in fully connected layers is that we apply the operation on a per-channel basis across all locations. This is compatible with our assumption of translation invariance that led to convolutions: we assumed that the specific location of a pattern within an image was not critical for the purpose of understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465989f-9f91-4871-8c6e-6666522e222b",
   "metadata": {},
   "source": [
    "## **Data**\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-iamges-idx3-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-iamges-idx3-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-iamges-idx1-ubyte.gz  \n",
    "`{root}\\FashionMNIST\\raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754ee39f-7a59-4d3e-9428-629e82aecf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Resize((224, 224)),  # upscale\n",
    "                            transforms.ToTensor()])\n",
    "\n",
    "data_train = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, transform=trans, download=False \n",
    ")\n",
    "data_val = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, transform=trans, download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1e6be1-a6ca-48e2-b2dd-94ae2bc76df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8d6941-74da-4500-8033-9c5c1c896c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6430b51-5f9c-4d2f-9dde-71f2c463d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "image, label = data_train[0]  # [image, label]\n",
    "print(image.shape) # (channel, height, weight)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e172f13-abd3-4213-a95e-bfcc652424a1",
   "metadata": {},
   "source": [
    "## **LeNet with BN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f18529-fc60-468f-8809-c683bdcdf74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNLenet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(out_channels=6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(out_channels=16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ad8ebb-8c25-4eca-9d0d-1a64a63254a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 220, 220]             156\n",
      "       BatchNorm2d-2          [-1, 6, 220, 220]              12\n",
      "           Sigmoid-3          [-1, 6, 220, 220]               0\n",
      "         AvgPool2d-4          [-1, 6, 110, 110]               0\n",
      "            Conv2d-5         [-1, 16, 106, 106]           2,416\n",
      "       BatchNorm2d-6         [-1, 16, 106, 106]              32\n",
      "           Sigmoid-7         [-1, 16, 106, 106]               0\n",
      "         AvgPool2d-8           [-1, 16, 53, 53]               0\n",
      "           Flatten-9                [-1, 44944]               0\n",
      "           Linear-10                  [-1, 120]       5,393,400\n",
      "      BatchNorm1d-11                  [-1, 120]             240\n",
      "          Sigmoid-12                  [-1, 120]               0\n",
      "           Linear-13                   [-1, 84]          10,164\n",
      "      BatchNorm1d-14                   [-1, 84]             168\n",
      "          Sigmoid-15                   [-1, 84]               0\n",
      "           Linear-16                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 5,407,438\n",
      "Trainable params: 5,407,438\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 12.01\n",
      "Params size (MB): 20.63\n",
      "Estimated Total Size (MB): 32.83\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(BNLenet(), input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259f054-b4e6-4622-9fe1-579b13e30fa3",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55510518-b2fc-4f88-8589-3acac1e03918",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efdb6f8-70ea-423d-bc32-28bcc8af7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BNLenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715b521f-5f26-47d3-93d3-149e04ec9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba4c346-4a86-441d-addb-fbad76f8226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    preds = y_hat.argmax(axis=1).type(y.dtype)  # (B)\n",
    "    compare = (preds == y).type(torch.float32)  # (B)\n",
    "    return compare.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347302c3-c92d-40ef-a916-074e45f58555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    for b, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "        if b % 10 == 0:\n",
    "            print(f'epoch={i} | batch={b} | train_loss={train_loss/num_train_batches:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        val_acc = 0\n",
    "        total = 0\n",
    "        for X, y in val_loader:\n",
    "            y_hat = model(X)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            val_acc += accuracy(y_hat, y)\n",
    "            total += y.numel()\n",
    "        \n",
    "    print(f'epoch={i} | train_loss={train_loss/num_train_batches:.4f} | val_loss={val_loss/num_val_batches:.4f} | val_acc={val_acc/total:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
