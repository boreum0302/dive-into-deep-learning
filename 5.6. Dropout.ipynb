{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb40935-f8f2-4fa0-a8d6-0ccf8b0b3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847c8b0-d124-4c22-8676-f70bd77ba31d",
   "metadata": {},
   "source": [
    "Typically, we disable dropout at test time. Given a trained model and a new example, we do not drop out any nodes and thus do not need to normalize. However, there are some exceptions: some researchers use dropout at test time as a heuristic for estimating the uncertainty of neural network predictions: if the predictions agree across many different dropout outputs, then we might say that the network is more confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc2ad34-9147-4100-a0ba-a808921cccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) > dropout).to(X.dtype)\n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da755aa9-5074-41ff-b5e4-dc82508baadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  0.,  0.,  6.,  8.,  0.,  0., 14.],\n",
      "        [ 0., 18.,  0.,  0.,  0.,  0., 28.,  0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((2, 8))\n",
    "print(dropout_layer(X, 0))\n",
    "print(dropout_layer(X, 0.5))\n",
    "print(dropout_layer(X, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de36ed2-8301-4e88-9784-dec72d79d8cf",
   "metadata": {},
   "source": [
    "## **Data**\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-iamges-idx3-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-iamges-idx3-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-iamges-idx1-ubyte.gz  \n",
    "`{root}\\FashionMNIST\\raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d44737-cea2-446a-a92e-5610015d6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Resize((32, 32)),  # upscale\n",
    "                            transforms.ToTensor()])\n",
    "\n",
    "data_train = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, transform=trans, download=False \n",
    ")\n",
    "data_val = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, transform=trans, download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac349cdd-0af7-4712-a532-b9e10666d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "image, label = data_train[0]  # [image, label]\n",
    "print(image.shape) # (channel, height, weight)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aff07b5-3f9b-49fa-b356-7a5fef0918cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f688f-0a2c-4a5c-bfc8-1b4a0021d4be",
   "metadata": {},
   "source": [
    "## **From Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac1b14f-efba-4534-811c-7eada9864d5a",
   "metadata": {},
   "source": [
    "A common choice is to set a lower dropout probability closer to the input layer. We ensure that dropout is only active during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3765b0f7-6178-4d79-a2c8-23b1bb1619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMLPScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens_1, num_hiddens_2, num_outputs, dropout_1, dropout_2):\n",
    "        super().__init__()\n",
    "        self.dropout_1 = dropout_1\n",
    "        self.dropout_2 = dropout_2\n",
    "        self.l1 = nn.Linear(num_inputs, num_hiddens_1)\n",
    "        self.l2 = nn.Linear(num_hiddens_1, num_hiddens_2)\n",
    "        self.l3 = nn.Linear(num_hiddens_2, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        H1 = self.relu(self.l1(X))\n",
    "        if self.training:  # model.train() 하면 self.training에 True가 assign됨\n",
    "            H1 = dropout_layer(H1, self.dropout_1)\n",
    "        H2 = self.relu(self.l2(H1))\n",
    "        if self.training:\n",
    "            H2 = dropout_layer(H2, self.dropout_2)\n",
    "        return self.softmax(self.l3(H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e60be1a-e959-42e3-8641-e86a4ddf1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DropoutMLPScratch(\n",
    "    num_inputs=1*32*32, num_hiddens_1=256, num_hiddens_2=256, num_outputs=10,\n",
    "    dropout_1=0.5, dropout_2=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9782c0eb-13a0-45a2-8c03-45f8fdb70d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "448a33f5-e3a2-4c58-a3e9-5aa27cb8ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    # sum -y_i*log(y_hat_i)\n",
    "    return -torch.log(y_hat[list(range(y_hat.shape[0])), y]).mean()  # 정의는 sum()인데 batch_size로 나눠주려고 mean() 씀\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    preds = y_hat.argmax(axis=1).type(y.dtype)  # (B)\n",
    "    compare = (preds == y).type(torch.float32)  # (B)\n",
    "    return compare.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93715fe9-57f4-4cbb-b2e6-73fc181fde49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train_loss=0.7897 | val_loss=0.5252 | val_acc=0.8064\n",
      "epoch=01 | train_loss=0.5323 | val_loss=0.4653 | val_acc=0.8287\n",
      "epoch=02 | train_loss=0.4842 | val_loss=0.4434 | val_acc=0.8385\n",
      "epoch=03 | train_loss=0.4528 | val_loss=0.4449 | val_acc=0.8358\n",
      "epoch=04 | train_loss=0.4359 | val_loss=0.3994 | val_acc=0.8562\n",
      "epoch=05 | train_loss=0.4209 | val_loss=0.4012 | val_acc=0.8578\n",
      "epoch=06 | train_loss=0.4096 | val_loss=0.3923 | val_acc=0.8557\n",
      "epoch=07 | train_loss=0.4013 | val_loss=0.3825 | val_acc=0.8603\n",
      "epoch=08 | train_loss=0.3937 | val_loss=0.3808 | val_acc=0.8560\n",
      "epoch=09 | train_loss=0.3814 | val_loss=0.3794 | val_acc=0.8588\n",
      "CPU times: total: 13min 2s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 10\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    model.train()  # dropout 켜짐\n",
    "    train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()  # paramter.grad에 저장된 값을 초기화\n",
    "        y_hat = model(X)\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        loss.backward()   # parameter.grad에 미분값이 assign됨\n",
    "        optimizer.step()  # parameter.grad에 저장된 값에 따라 paramter의 값을 update 해줌\n",
    "        train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    model.eval()  # dropout 꺼짐\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    num_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            y_hat = model(X)\n",
    "            loss = cross_entropy(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            val_acc += accuracy(y_hat, y)\n",
    "\n",
    "    print(f'epoch={i:02d} | train_loss={train_loss/num_train_batches:.4f} | val_loss={val_loss/num_val_batches:.4f} | val_acc={val_acc/num_val_batches:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8def8bd-adbc-4540-b9e1-d71d9c018ef1",
   "metadata": {},
   "source": [
    "## **Concise Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad07c260-8a24-4d53-b9aa-e2fe8e360dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens_1, num_hiddens_2, num_outputs, dropout_1, dropout_2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_1),\n",
    "            nn.Linear(num_hiddens_1, num_hiddens_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_2),\n",
    "            nn.Linear(num_hiddens_2, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e0086e-88ca-4e28-a740-55663dc408a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DropoutMLP(\n",
    "    num_inputs=1*32*32, num_hiddens_1=256, num_hiddens_2=256, num_outputs=10,\n",
    "    dropout_1=0.5, dropout_2=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d952672c-321b-4606-bcee-facd06cc048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125f2b0b-a0dd-4e43-a16a-f4881b9f479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    return loss_fn(y_hat, y)  # 여기서의 y는 soft max 씌운 확률 값이 아니라 output 그 자체\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    preds = y_hat.argmax(axis=1).type(y.dtype)  # (B)\n",
    "    compare = (preds == y).type(torch.float32)  # (B)\n",
    "    return compare.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34ecafff-b8fd-459d-821e-1cf3a027e5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train_loss=0.7975 | val_loss=0.5321 | val_acc=0.8018\n",
      "epoch=01 | train_loss=0.5361 | val_loss=0.4685 | val_acc=0.8260\n",
      "epoch=02 | train_loss=0.4828 | val_loss=0.4246 | val_acc=0.8486\n",
      "epoch=03 | train_loss=0.4561 | val_loss=0.4564 | val_acc=0.8344\n",
      "epoch=04 | train_loss=0.4361 | val_loss=0.4073 | val_acc=0.8526\n",
      "epoch=05 | train_loss=0.4202 | val_loss=0.4010 | val_acc=0.8518\n",
      "epoch=06 | train_loss=0.4087 | val_loss=0.3861 | val_acc=0.8583\n",
      "epoch=07 | train_loss=0.3976 | val_loss=0.3791 | val_acc=0.8627\n",
      "epoch=08 | train_loss=0.3893 | val_loss=0.3986 | val_acc=0.8557\n",
      "epoch=09 | train_loss=0.3809 | val_loss=0.3614 | val_acc=0.8658\n",
      "CPU times: total: 14min 3s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 10\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    model.train()  # dropout 켜짐\n",
    "    train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()  # paramter.grad에 저장된 값을 초기화\n",
    "        y_hat = model(X)\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        loss.backward()   # parameter.grad에 미분값이 assign됨\n",
    "        optimizer.step()  # parameter.grad에 저장된 값에 따라 paramter의 값을 update 해줌\n",
    "        train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    model.eval()  # dropout 꺼짐\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    num_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            y_hat = model(X)\n",
    "            loss = cross_entropy(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            val_acc += accuracy(y_hat, y)\n",
    "\n",
    "    print(f'epoch={i:02d} | train_loss={train_loss/num_train_batches:.4f} | val_loss={val_loss/num_val_batches:.4f} | val_acc={val_acc/num_val_batches:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
