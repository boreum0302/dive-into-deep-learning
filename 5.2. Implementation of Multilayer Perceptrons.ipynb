{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c4a627-6ec2-4e1e-9612-62a3ea4dd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679e507-04c4-4680-9e62-9562a64205d5",
   "metadata": {},
   "source": [
    "## **Data**\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-iamges-idx3-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-iamges-idx3-ubyte.gz  \n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-iamges-idx1-ubyte.gz  \n",
    "`{root}\\FashionMNIST\\raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310504d3-66ab-4e96-a34e-d03ba8cae93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Resize((32, 32)),  # upscale\n",
    "                            transforms.ToTensor()])\n",
    "\n",
    "data_train = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, transform=trans, download=False \n",
    ")\n",
    "data_val = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, transform=trans, download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547a6706-7a57-4eb1-9bfb-2de9070dcfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "image, label = data_train[0]  # [image, label]\n",
    "print(image.shape) # (channel, height, weight)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5c9cdb-161c-4e3c-b50d-80c098c987be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae45d69-51c9-408b-ac99-0b3eb822967f",
   "metadata": {},
   "source": [
    "## **From Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9040058-ec73-4584-876e-84156cab6ab3",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60686aae-c1a6-4b23-bbb8-fc5b51df3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)\n",
    "\n",
    "def softmax(X):  # X.shape = (n, d)\n",
    "    X_exp = torch.exp(X)  # elementwise\n",
    "    partition = X_exp.sum(1, keepdims=True)  # shape: (n, 1)\n",
    "    return X_exp / partition  # shape: (n, 1)\n",
    "\n",
    "class MLPScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n",
    "        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n",
    "        self.b2 = nn.Parameter(torch.zeros(num_outputs))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.reshape(-1, self.num_inputs)\n",
    "        H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "        O = torch.matmul(H, self.W2) + self.b2\n",
    "        return softmax(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fe1f9-57db-4121-be21-64ab4a6b4f11",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d05e99-62cf-4d72-b68a-26c7fb07696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    # sum -y_i*log(y_hat_i)\n",
    "    return -torch.log(y_hat[list(range(y_hat.shape[0])), y]).mean()  # 정의는 sum()인데 batch_size로 나눠주려고 mean() 씀\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    preds = y_hat.argmax(axis=1).type(y.dtype)  # (B)\n",
    "    compare = (preds == y).type(torch.float32)  # (B)\n",
    "    return compare.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b14a34-2d6a-4b4e-ba17-062207adb187",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e094e45-e706-4982-b5b3-d5f239a9945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "model = MLPScratch(num_inputs=1*32*32, num_outputs=10, num_hiddens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da14482-da39-4c57-97e1-267f2838d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372fb7d0-2756-4d6f-906c-6a890809917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train_loss=0.6806 | val_loss=0.6302 | val_acc=0.7959\n",
      "epoch=01 | train_loss=0.4514 | val_loss=0.4564 | val_acc=0.8349\n",
      "epoch=02 | train_loss=0.4035 | val_loss=0.5585 | val_acc=0.7727\n",
      "epoch=03 | train_loss=0.3772 | val_loss=0.4152 | val_acc=0.8489\n",
      "epoch=04 | train_loss=0.3583 | val_loss=0.3967 | val_acc=0.8560\n",
      "epoch=05 | train_loss=0.3421 | val_loss=0.4904 | val_acc=0.8115\n",
      "epoch=06 | train_loss=0.3286 | val_loss=0.4114 | val_acc=0.8486\n",
      "epoch=07 | train_loss=0.3176 | val_loss=0.3660 | val_acc=0.8678\n",
      "epoch=08 | train_loss=0.3101 | val_loss=0.3547 | val_acc=0.8732\n",
      "epoch=09 | train_loss=0.2994 | val_loss=0.3454 | val_acc=0.8745\n",
      "CPU times: total: 12min 56s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 10\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()  # paramter.grad에 저장된 값을 None으로 초기화\n",
    "        y_hat = model(X)\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    num_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            y_hat = model(X)\n",
    "            loss = cross_entropy(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            val_acc += accuracy(y_hat, y)\n",
    "\n",
    "    print(f'epoch={i:02d} | train_loss={train_loss/num_train_batches:.4f} | val_loss={val_loss/num_val_batches:.4f} | val_acc={val_acc/num_val_batches:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e4731-04ac-4c94-9b61-e34a63ac8adb",
   "metadata": {},
   "source": [
    "## **Concise Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7e59437-36d6-4f5f-bb3f-c2dcd49b2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hiddens, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.reshape(-1, self.num_inputs)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d9b669c-6a22-4ca8-b04d-55e87d2cfa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    # sum -y_i*log(y_hat_i)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn(y_hat, y)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    # y_hat: (B, q)\n",
    "    # y: (B)\n",
    "    preds = y_hat.argmax(axis=1).type(y.dtype)  # (B)\n",
    "    compare = (preds == y).type(torch.float32)  # (B)\n",
    "    return compare.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a321f169-3613-4712-bc5b-ecaadf1af35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "model = MLP(num_inputs=1*32*32, num_hiddens=256, num_outputs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55c9198b-821d-4acf-a08d-a1318f754bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d98bea0b-fc67-4384-9f86-608265565637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train_loss=0.6254 | val_loss=0.5637 | val_acc=0.8024\n",
      "epoch=01 | train_loss=0.4408 | val_loss=0.4310 | val_acc=0.8487\n",
      "epoch=02 | train_loss=0.3976 | val_loss=0.4477 | val_acc=0.8359\n",
      "epoch=03 | train_loss=0.3724 | val_loss=0.4297 | val_acc=0.8419\n",
      "epoch=04 | train_loss=0.3510 | val_loss=0.3722 | val_acc=0.8651\n",
      "epoch=05 | train_loss=0.3384 | val_loss=0.3619 | val_acc=0.8686\n",
      "epoch=06 | train_loss=0.3237 | val_loss=0.3704 | val_acc=0.8637\n",
      "epoch=07 | train_loss=0.3137 | val_loss=0.3483 | val_acc=0.8776\n",
      "epoch=08 | train_loss=0.3059 | val_loss=0.3483 | val_acc=0.8748\n",
      "epoch=09 | train_loss=0.2974 | val_loss=0.3666 | val_acc=0.8648\n",
      "CPU times: total: 12min 7s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 10\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()  # paramter.grad에 저장된 값을 None으로 초기화\n",
    "        y_hat = model(X)\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    num_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            y_hat = model(X)\n",
    "            loss = cross_entropy(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            val_acc += accuracy(y_hat, y)\n",
    "\n",
    "    print(f'epoch={i:02d} | train_loss={train_loss/num_train_batches:.4f} | val_loss={val_loss/num_val_batches:.4f} | val_acc={val_acc/num_val_batches:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
